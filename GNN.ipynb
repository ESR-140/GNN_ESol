{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rdkit\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Load the dataset\n",
    "def LoadData():\n",
    "    data = MoleculeNet(root=\".\", name=\"ESOL\")\n",
    "    molecule = Chem.MolFromSmiles(data[0][\"smiles\"])\n",
    "    return data, molecule\n",
    "\n",
    "def WrapData_toLoader(data,\n",
    "                      data_size,\n",
    "                      NUM_GRAPHS_PER_BATCH=64,\n",
    "                      split_ratio=0.8):\n",
    "    \n",
    "    loader = DataLoader(data[:int(data_size *split_ratio)],\n",
    "                        batch_size = NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "    test_loader = DataLoader(data[int(data_size*split_ratio):],\n",
    "                            batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "    return loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, molecule = LoadData()\n",
    "MoleculeNet(root=\".\", name=\"ESOL\").num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "\n",
    "\n",
    "embedding_size = 64\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = Linear(embedding_size*2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "          \n",
    "        # Global Pooling (stack different aggregations)\n",
    "        # \n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.out(hidden)\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "\n",
    "#Define Loss Function and Optimizer to be used\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "#Load the Data\n",
    "data_size = len(data)\n",
    "loader, test_loader = WrapData_toLoader(data, data_size)\n",
    "\n",
    "#Define function the train the Data\n",
    "\n",
    "def train(data):\n",
    "    #Enumerate through each batch\n",
    "    for batch in loader:\n",
    "        batch.to(device)  #Use GPU\n",
    "        optimizer.zero_grad() #Reset the gradients for each batch\n",
    "        #Passing node features and connection info\n",
    "        pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "        #Calculating the loss and gradients\n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()\n",
    "        #Update using the gradients\n",
    "        optimizer.step()\n",
    "    return loss, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.molecule_net.MoleculeNet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training......\n",
      "Epoch 1 / 1000  |  Train Loss: 11.665949821472168\n",
      "Epoch 2 / 1000  |  Train Loss: 10.07497501373291\n",
      "Epoch 3 / 1000  |  Train Loss: 2.6209933757781982\n",
      "Epoch 4 / 1000  |  Train Loss: 4.1252760887146\n",
      "Epoch 5 / 1000  |  Train Loss: 1.7706319093704224\n",
      "Epoch 6 / 1000  |  Train Loss: 2.27337908744812\n",
      "Epoch 7 / 1000  |  Train Loss: 2.6324708461761475\n",
      "Epoch 8 / 1000  |  Train Loss: 2.713014602661133\n",
      "Epoch 9 / 1000  |  Train Loss: 6.059814453125\n",
      "Epoch 10 / 1000  |  Train Loss: 2.9248058795928955\n",
      "Epoch 11 / 1000  |  Train Loss: 1.592408537864685\n",
      "Epoch 12 / 1000  |  Train Loss: 8.007978439331055\n",
      "Epoch 13 / 1000  |  Train Loss: 0.7166104316711426\n",
      "Epoch 14 / 1000  |  Train Loss: 7.643935680389404\n",
      "Epoch 15 / 1000  |  Train Loss: 4.460394859313965\n",
      "Epoch 16 / 1000  |  Train Loss: 6.1365203857421875\n",
      "Epoch 17 / 1000  |  Train Loss: 0.8607726693153381\n",
      "Epoch 18 / 1000  |  Train Loss: 0.9701941609382629\n",
      "Epoch 19 / 1000  |  Train Loss: 11.737007141113281\n",
      "Epoch 20 / 1000  |  Train Loss: 2.0268237590789795\n",
      "Epoch 21 / 1000  |  Train Loss: 1.9809781312942505\n",
      "Epoch 22 / 1000  |  Train Loss: 1.188370704650879\n",
      "Epoch 23 / 1000  |  Train Loss: 2.0363824367523193\n",
      "Epoch 24 / 1000  |  Train Loss: 2.303828001022339\n",
      "Epoch 25 / 1000  |  Train Loss: 4.269286632537842\n",
      "Epoch 26 / 1000  |  Train Loss: 3.483330488204956\n",
      "Epoch 27 / 1000  |  Train Loss: 1.8451563119888306\n",
      "Epoch 28 / 1000  |  Train Loss: 2.057075262069702\n",
      "Epoch 29 / 1000  |  Train Loss: 1.9614229202270508\n",
      "Epoch 30 / 1000  |  Train Loss: 0.3311420977115631\n",
      "Epoch 31 / 1000  |  Train Loss: 1.888245940208435\n",
      "Epoch 32 / 1000  |  Train Loss: 3.467308282852173\n",
      "Epoch 33 / 1000  |  Train Loss: 0.4925326406955719\n",
      "Epoch 34 / 1000  |  Train Loss: 2.532299518585205\n",
      "Epoch 35 / 1000  |  Train Loss: 1.852257251739502\n",
      "Epoch 36 / 1000  |  Train Loss: 0.5528407692909241\n",
      "Epoch 37 / 1000  |  Train Loss: 2.217449188232422\n",
      "Epoch 38 / 1000  |  Train Loss: 1.414493203163147\n",
      "Epoch 39 / 1000  |  Train Loss: 1.61573326587677\n",
      "Epoch 40 / 1000  |  Train Loss: 0.46930304169654846\n",
      "Epoch 41 / 1000  |  Train Loss: 2.667490243911743\n",
      "Epoch 42 / 1000  |  Train Loss: 1.0646650791168213\n",
      "Epoch 43 / 1000  |  Train Loss: 1.8948580026626587\n",
      "Epoch 44 / 1000  |  Train Loss: 0.9212260246276855\n",
      "Epoch 45 / 1000  |  Train Loss: 2.1751391887664795\n",
      "Epoch 46 / 1000  |  Train Loss: 0.7761874198913574\n",
      "Epoch 47 / 1000  |  Train Loss: 2.8052377700805664\n",
      "Epoch 48 / 1000  |  Train Loss: 1.8046232461929321\n",
      "Epoch 49 / 1000  |  Train Loss: 0.9845173954963684\n",
      "Epoch 50 / 1000  |  Train Loss: 0.500221312046051\n",
      "Epoch 51 / 1000  |  Train Loss: 2.0759785175323486\n",
      "Epoch 52 / 1000  |  Train Loss: 1.1442606449127197\n",
      "Epoch 53 / 1000  |  Train Loss: 0.392164945602417\n",
      "Epoch 54 / 1000  |  Train Loss: 3.9880599975585938\n",
      "Epoch 55 / 1000  |  Train Loss: 2.9279439449310303\n",
      "Epoch 56 / 1000  |  Train Loss: 1.1503838300704956\n",
      "Epoch 57 / 1000  |  Train Loss: 1.399507999420166\n",
      "Epoch 58 / 1000  |  Train Loss: 1.3342808485031128\n",
      "Epoch 59 / 1000  |  Train Loss: 0.23152947425842285\n",
      "Epoch 60 / 1000  |  Train Loss: 1.4539686441421509\n",
      "Epoch 61 / 1000  |  Train Loss: 0.12294188141822815\n",
      "Epoch 62 / 1000  |  Train Loss: 0.7836132049560547\n",
      "Epoch 63 / 1000  |  Train Loss: 1.7380765676498413\n",
      "Epoch 64 / 1000  |  Train Loss: 0.5946530699729919\n",
      "Epoch 65 / 1000  |  Train Loss: 1.0149517059326172\n",
      "Epoch 66 / 1000  |  Train Loss: 3.9340641498565674\n",
      "Epoch 67 / 1000  |  Train Loss: 0.7673967480659485\n",
      "Epoch 68 / 1000  |  Train Loss: 0.8964176177978516\n",
      "Epoch 69 / 1000  |  Train Loss: 1.5340012311935425\n",
      "Epoch 70 / 1000  |  Train Loss: 1.0630604028701782\n",
      "Epoch 71 / 1000  |  Train Loss: 0.4191869795322418\n",
      "Epoch 72 / 1000  |  Train Loss: 1.0498636960983276\n",
      "Epoch 73 / 1000  |  Train Loss: 1.4989224672317505\n",
      "Epoch 74 / 1000  |  Train Loss: 0.9860436320304871\n",
      "Epoch 75 / 1000  |  Train Loss: 0.42201876640319824\n",
      "Epoch 76 / 1000  |  Train Loss: 1.1575249433517456\n",
      "Epoch 77 / 1000  |  Train Loss: 1.2252801656723022\n",
      "Epoch 78 / 1000  |  Train Loss: 1.7444539070129395\n",
      "Epoch 79 / 1000  |  Train Loss: 1.1529046297073364\n",
      "Epoch 80 / 1000  |  Train Loss: 1.0153242349624634\n",
      "Epoch 81 / 1000  |  Train Loss: 1.1419072151184082\n",
      "Epoch 82 / 1000  |  Train Loss: 0.911309003829956\n",
      "Epoch 83 / 1000  |  Train Loss: 2.117371082305908\n",
      "Epoch 84 / 1000  |  Train Loss: 1.2637630701065063\n",
      "Epoch 85 / 1000  |  Train Loss: 0.47021007537841797\n",
      "Epoch 86 / 1000  |  Train Loss: 1.3908820152282715\n",
      "Epoch 87 / 1000  |  Train Loss: 0.4982033669948578\n",
      "Epoch 88 / 1000  |  Train Loss: 1.790502905845642\n",
      "Epoch 89 / 1000  |  Train Loss: 1.7911831140518188\n",
      "Epoch 90 / 1000  |  Train Loss: 0.4319402873516083\n",
      "Epoch 91 / 1000  |  Train Loss: 1.6699835062026978\n",
      "Epoch 92 / 1000  |  Train Loss: 1.47384774684906\n",
      "Epoch 93 / 1000  |  Train Loss: 2.345177173614502\n",
      "Epoch 94 / 1000  |  Train Loss: 0.7812926173210144\n",
      "Epoch 95 / 1000  |  Train Loss: 0.6999742984771729\n",
      "Epoch 96 / 1000  |  Train Loss: 0.5986794233322144\n",
      "Epoch 97 / 1000  |  Train Loss: 0.8164189457893372\n",
      "Epoch 98 / 1000  |  Train Loss: 2.6850547790527344\n",
      "Epoch 99 / 1000  |  Train Loss: 1.3519835472106934\n",
      "Epoch 100 / 1000  |  Train Loss: 3.816131830215454\n",
      "Epoch 101 / 1000  |  Train Loss: 0.8494141697883606\n",
      "Epoch 102 / 1000  |  Train Loss: 1.3356074094772339\n",
      "Epoch 103 / 1000  |  Train Loss: 1.6240010261535645\n",
      "Epoch 104 / 1000  |  Train Loss: 0.2483241707086563\n",
      "Epoch 105 / 1000  |  Train Loss: 1.3469347953796387\n",
      "Epoch 106 / 1000  |  Train Loss: 1.3252023458480835\n",
      "Epoch 107 / 1000  |  Train Loss: 0.4662277400493622\n",
      "Epoch 108 / 1000  |  Train Loss: 0.7372449040412903\n",
      "Epoch 109 / 1000  |  Train Loss: 2.224139451980591\n",
      "Epoch 110 / 1000  |  Train Loss: 0.27372464537620544\n",
      "Epoch 111 / 1000  |  Train Loss: 5.188423156738281\n",
      "Epoch 112 / 1000  |  Train Loss: 0.35849225521087646\n",
      "Epoch 113 / 1000  |  Train Loss: 1.000282645225525\n",
      "Epoch 114 / 1000  |  Train Loss: 1.4109426736831665\n",
      "Epoch 115 / 1000  |  Train Loss: 1.601933479309082\n",
      "Epoch 116 / 1000  |  Train Loss: 1.0255093574523926\n",
      "Epoch 117 / 1000  |  Train Loss: 1.3341151475906372\n",
      "Epoch 118 / 1000  |  Train Loss: 0.5817672610282898\n",
      "Epoch 119 / 1000  |  Train Loss: 1.1658560037612915\n",
      "Epoch 120 / 1000  |  Train Loss: 0.9280763268470764\n",
      "Epoch 121 / 1000  |  Train Loss: 0.5344477295875549\n",
      "Epoch 122 / 1000  |  Train Loss: 1.9881149530410767\n",
      "Epoch 123 / 1000  |  Train Loss: 1.019946575164795\n",
      "Epoch 124 / 1000  |  Train Loss: 1.459276795387268\n",
      "Epoch 125 / 1000  |  Train Loss: 1.1446508169174194\n",
      "Epoch 126 / 1000  |  Train Loss: 0.820350170135498\n",
      "Epoch 127 / 1000  |  Train Loss: 2.05607008934021\n",
      "Epoch 128 / 1000  |  Train Loss: 0.7947461009025574\n",
      "Epoch 129 / 1000  |  Train Loss: 0.265044629573822\n",
      "Epoch 130 / 1000  |  Train Loss: 0.7579144835472107\n",
      "Epoch 131 / 1000  |  Train Loss: 1.0126856565475464\n",
      "Epoch 132 / 1000  |  Train Loss: 0.4319535791873932\n",
      "Epoch 133 / 1000  |  Train Loss: 1.986262321472168\n",
      "Epoch 134 / 1000  |  Train Loss: 0.3023674190044403\n",
      "Epoch 135 / 1000  |  Train Loss: 0.8243474364280701\n",
      "Epoch 136 / 1000  |  Train Loss: 0.3862191140651703\n",
      "Epoch 137 / 1000  |  Train Loss: 1.1944125890731812\n",
      "Epoch 138 / 1000  |  Train Loss: 0.9070383906364441\n",
      "Epoch 139 / 1000  |  Train Loss: 0.5667787790298462\n",
      "Epoch 140 / 1000  |  Train Loss: 0.47973838448524475\n",
      "Epoch 141 / 1000  |  Train Loss: 0.11199846118688583\n",
      "Epoch 142 / 1000  |  Train Loss: 0.6621480584144592\n",
      "Epoch 143 / 1000  |  Train Loss: 0.9147145748138428\n",
      "Epoch 144 / 1000  |  Train Loss: 1.4196171760559082\n",
      "Epoch 145 / 1000  |  Train Loss: 4.7127461433410645\n",
      "Epoch 146 / 1000  |  Train Loss: 0.595278799533844\n",
      "Epoch 147 / 1000  |  Train Loss: 2.1027235984802246\n",
      "Epoch 148 / 1000  |  Train Loss: 3.047996759414673\n",
      "Epoch 149 / 1000  |  Train Loss: 0.39991962909698486\n",
      "Epoch 150 / 1000  |  Train Loss: 4.473113059997559\n",
      "Epoch 151 / 1000  |  Train Loss: 0.2744641602039337\n",
      "Epoch 152 / 1000  |  Train Loss: 0.7956061959266663\n",
      "Epoch 153 / 1000  |  Train Loss: 0.8926262259483337\n",
      "Epoch 154 / 1000  |  Train Loss: 0.2571663558483124\n",
      "Epoch 155 / 1000  |  Train Loss: 0.5209540724754333\n",
      "Epoch 156 / 1000  |  Train Loss: 0.7349205613136292\n",
      "Epoch 157 / 1000  |  Train Loss: 1.1369041204452515\n",
      "Epoch 158 / 1000  |  Train Loss: 1.7113996744155884\n",
      "Epoch 159 / 1000  |  Train Loss: 1.3546432256698608\n",
      "Epoch 160 / 1000  |  Train Loss: 0.6063836216926575\n",
      "Epoch 161 / 1000  |  Train Loss: 4.334167003631592\n",
      "Epoch 162 / 1000  |  Train Loss: 2.129018545150757\n",
      "Epoch 163 / 1000  |  Train Loss: 0.21346275508403778\n",
      "Epoch 164 / 1000  |  Train Loss: 0.4951643645763397\n",
      "Epoch 165 / 1000  |  Train Loss: 0.20695680379867554\n",
      "Epoch 166 / 1000  |  Train Loss: 0.8775967955589294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 6\u001b[0m     loss,h \u001b[39m=\u001b[39m train(data)\n\u001b[0;32m      7\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m  |  Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m#Calculating the loss and gradients\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, batch\u001b[39m.\u001b[39my)\n\u001b[1;32m---> 26\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     27\u001b[0m \u001b[39m#Update using the gradients\u001b[39;00m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the data\n",
    "num_epochs = 1000\n",
    "print(\"Start Training......\")\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss,h = train(data)\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch+1} / {num_epochs}  |  Train Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the training loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "losses_float = [float(loss.cpu().detach().numpy()) for loss in losses] \n",
    "loss_indices = [i for i,l in enumerate(losses_float)] \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "plt.plot(loss_indices, losses_float)\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a prediction\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(columns=[\"y_real\", \"y_pred\"]) # create an empty DataFrame to store results\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_loader: # iterate over all batches in test_loader\n",
    "        test_batch.to(device)\n",
    "        pred, embed = model(test_batch.x.float(), test_batch.edge_index, test_batch.batch)\n",
    "        df = pd.DataFrame()\n",
    "        df[\"y_real\"] = test_batch.y.tolist()\n",
    "        df[\"y_pred\"] = pred.tolist()\n",
    "        df[\"y_real\"] = df[\"y_real\"].apply(lambda row: row[0])\n",
    "        df[\"y_pred\"] = df[\"y_pred\"].apply(lambda row: row[0])\n",
    "        results = pd.concat([results, df]) # concatenate the results into a single DataFrame\n",
    "results.reset_index(drop=True, inplace=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Diff\"] = results[\"y_real\"]-results[\"y_pred\"]\n",
    "results[\"Diff Sq\"] = results[\"Diff\"].apply(lambda x: x*x)\n",
    "test_mse = results[\"Diff Sq\"].mean()\n",
    "print(f\"Test MSE Error is {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.scatter(results.loc[:,\"y_real\"],results.loc[:,\"y_pred\"],marker ='.')\n",
    "plt.xlabel(\"Actual Data\")\n",
    "plt.ylabel(\"Predicted Data\")\n",
    "line = np.linspace(1.2*results[\"y_real\"].min(),1.2*results[\"y_real\"].max())\n",
    "plt.plot(line,line,'-r')\n",
    "plt.show\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
